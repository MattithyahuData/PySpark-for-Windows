# [How to Install and Run PySpark in Jupyter Notebook on Windows](https://changhsinlee.com/install-pyspark-windows-jupyter/)
*   Assuming you already have Anaconda installed 
*   Some links may be outdated or change slightly but the process still stands 


1. [Install Spark](https://spark.apache.org/downloads.html)
    * Leave default Spark release and package type, the most recent versions will be selected by default.  
    * Choose a package type: Download Spark: [spark-3.2.1-bin-hadoop3.2.tgz](spark-3.2.1-bin-hadoop3.2.tgz)  Follow download link 
    * Click on first link on new page to download   

2. [Download Java dk 8](https://www.oracle.com/in/java/technologies/javase/downloads/#java8) 

3. [Download winutils from this Github Repo](https://github.com/cdarlint/winutils)

4. pip install pyspark, findspark


[Reference article](https://changhsinlee.com/install-pyspark-windows-jupyter/)